{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Libraries and API-Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = input(\"Enter your API-Key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset Selection and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57 pages from the document.\n"
     ]
    }
   ],
   "source": [
    "# Load the document\n",
    "document_dir = \"data/\"\n",
    "filename = \"BiologicalReviews-2024-Kershenbaum-Automaticdetectionforbioacousticresearchapracticalguidefromandfor.pdf\"\n",
    "file_path = os.path.join(document_dir, filename)\n",
    "\n",
    "# Use PyPDFLoader to load the PDF\n",
    "pages = PyPDFLoader(file_path).load_and_split()\n",
    "print(f\"Loaded {len(pages)} pages from the document.\")\n",
    "\n",
    "# Initialize CharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=10)\n",
    "\n",
    "# Split documents\n",
    "docs = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embeddings function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\", openai_api_key=API_KEY) # You can find the different models in openai.com, under Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load it into Chroma\n",
    "db = Chroma.from_documents(documents=docs,\n",
    "                           embedding=embeddings,\n",
    "                           persist_directory=\"./chroma_db\"\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User question\n",
    "user_question = \"How can I transform acoustic data?\"\n",
    "\n",
    "# Build the context\n",
    "def build_context(question):\n",
    "    retrieved_docs = db.similarity_search(query=question, # Consider similarity_search_with_score to chose k depending on the distance between each consecutive doc \n",
    "                                      k=5 # Number of relevant retrieved docs\n",
    "                                      )\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs]) # Combine documents into a single string to prepare the context\n",
    "    return context\n",
    "\n",
    "# Build the prompt\n",
    "def build_prompt(question, context):\n",
    "    prompt = f\"\"\"\n",
    "    ## INTRODUCTION\n",
    "    You are a Chatbot designed to help answer technical questions based on information in a scientific document.\n",
    "\n",
    "    ## USER QUESTION\n",
    "    The user asked: \"{question}\"\n",
    "\n",
    "    ## CONTEXT\n",
    "    Technical Documentation for the software:\n",
    "    '''\n",
    "    {context}\n",
    "    '''\n",
    "\n",
    "    ## RESTRICTIONS\n",
    "    - Use only factual information from the provided context.\n",
    "    - If you cannot find the answer, state clearly that you do not know based on the available context.\n",
    "    - Focus strictly on bioacoustics-related content and avoid any information outside the provided document.\n",
    "    - Avoid speculation, opinions, and subjectivity.\n",
    "    - Keep the response formal, clear, and concise, without unnecessary details.\n",
    "\n",
    "    ## TASK\n",
    "    1. Directly answer the user's question.\n",
    "    2. Mention specific sections or page numbers if relevant information is available in particular parts of the document.\n",
    "    3. Format the answer in Markdown.\n",
    "\n",
    "    ## RESPONSE STRUCTURE:\n",
    "    '''\n",
    "    # [Answer Title]\n",
    "    [answer text]\n",
    "\n",
    "\n",
    "\n",
    "    ## CONVERSATION:\n",
    "    User: {question}\n",
    "    Agent:\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = build_context(user_question)\n",
    "prompt = build_prompt(user_question, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Transforming Acoustic Data\n",
       "To transform acoustic data, one can utilize data augmentation techniques to artificially increase variability in the dataset. The choice of augmentation techniques should be aligned with the specific application and must cover the range of variations found in real signals. However, it is crucial to avoid transformations that may invalidate annotations, such as reversing sounds in a bird call detector, which could lead to confusion between species calls. \n",
       "\n",
       "Additionally, the dominant approach in bioacoustics involves using spectral representations like spectrograms or mel-spectrograms, which facilitate the visualization of acoustic data and enable the use of vision-based models such as Convolutional Neural Networks (CNNs). It is important to note that while these representations are useful, some information from the raw waveform may be lost, particularly for transient signals.\n",
       "\n",
       "Source:\n",
       "â€¢ Based on content from pages 1-2 in the document titled \"Automatic Detection for Bioacoustic Research: A Practical Guide\""
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client = OpenAI(api_key = API_KEY)\n",
    "\n",
    "# Prepare the messages payload\n",
    "messages = [{'role': 'user', 'content': prompt}]\n",
    "\n",
    "# Set model parameters\n",
    "model_params = {'model': \"gpt-4o-mini\",\n",
    "                'temperature': 0.4,\n",
    "                'max_tokens': 3000\n",
    "                }\n",
    "\n",
    "chat_completion = client.chat.completions.create(messages = messages,\n",
    "                                                 **model_params,\n",
    "                                                 timeout=120\n",
    "                                                 )\n",
    "\n",
    "answer = chat_completion.choices[0].message.content\n",
    "display(Markdown(answer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the importance of automatic detection ...</td>\n",
       "      <td>Automatic detection in bioacoustic research pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does automatic detection assist in monitor...</td>\n",
       "      <td>Automatic detection facilitates ecosystem moni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What challenges are faced in implementing auto...</td>\n",
       "      <td>Challenges in implementing automatic detection...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How do machine learning algorithms contribute ...</td>\n",
       "      <td>Machine learning algorithms analyze vast amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are some real-world applications of autom...</td>\n",
       "      <td>Automatic detection is widely applied in conse...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0  What is the importance of automatic detection ...   \n",
       "1  How does automatic detection assist in monitor...   \n",
       "2  What challenges are faced in implementing auto...   \n",
       "3  How do machine learning algorithms contribute ...   \n",
       "4  What are some real-world applications of autom...   \n",
       "\n",
       "                                              Answer  \n",
       "0  Automatic detection in bioacoustic research pl...  \n",
       "1  Automatic detection facilitates ecosystem moni...  \n",
       "2  Challenges in implementing automatic detection...  \n",
       "3  Machine learning algorithms analyze vast amoun...  \n",
       "4  Automatic detection is widely applied in conse...  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test questions\n",
    "test_questions = pd.read_csv(\"data/bioacoustic_questions_answers.csv\")\n",
    "test_questions = test_questions[:5]\n",
    "test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate answers to the questions\n",
    "predictions = []\n",
    "for question in test_questions[\"Question\"]:\n",
    "    context = build_context(question)\n",
    "    prompt = build_prompt(question,context)\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    chat_completion = client.chat.completions.create(messages = messages,\n",
    "                                                    **model_params,\n",
    "                                                    timeout=120\n",
    "                                                    )\n",
    "    predictions.append(chat_completion.choices[0].message.content)\n",
    "predictions = pd.Series(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         # Importance of Automatic Detection in Bioacoustic Research\\n\\nAutomatic detection is crucial in bioacoustic research as it allows researchers to scale their studies to larger spatiotemporal data sets, enhancing monitoring, spatial habitat use, and behavioral analysis. It significantly accelerates acoustic monitoring by automating the extraction of sounds of interest, which traditionally requires extensive manual effort and expertise. This automation reduces delays between data collection and analysis, providing timely results that are especially important in conservation biology. Furthermore, it addresses challenges related to the expertise gap between biological sciences and machine learning, facilitating broader accessibility and usability of bioacoustic research tools.\\n\\nThe document highlights these points in sections discussing the current state of automatic detection and its applications in biological sciences.\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              # Automatic Detection in Ecosystem Monitoring\\n\\nAutomatic detection enhances the efficiency and efficacy of monitoring ecosystems by enabling density estimation, occupancy assessments, species presence identification, and phenology tracking, such as the timing of breeding or daily song onset. This technology can be integrated with non-invasive monitoring methods, including camera traps and scat surveys, which aids in gathering additional information and monitoring cryptic species that might otherwise go undetected. Furthermore, ongoing collaboration between biologists and computer scientists improves survey monitoring, spatial habitat use, and behavioral analysis, allowing researchers to scale their studies to larger spatiotemporal datasets (Section 1, \"Ecosystems and acoustic indices\").\n",
       "2    # Challenges in Implementing Automatic Detection Systems\\n\\nThe implementation of automatic detection systems in biological studies faces several significant challenges, including:\\n\\n1. **Infrastructure Limitations**: There may be inadequate infrastructure to support the necessary technology and data processing capabilities.\\n   \\n2. **Lack of Training**: Researchers may not have sufficient training in the technical skills required for coding and machine learning (ML), which are essential for developing and utilizing these systems.\\n\\n3. **Inaccessibility of Methods**: Many methods may not be readily accessible, particularly for researchers in the Global South, who may also face challenges like lack of access to high-speed internet for cloud storage and computing.\\n\\n4. **Recording Quality Issues**: Field recordings often suffer from suboptimal quality and low signal-to-noise ratio (SNR), which can hinder detection capabilities.\\n\\n5. **Variability of Acoustic Signals**: Acoustic signals can be highly variable and irregular, with low stereotypy and significant differences between individuals, groups, or geographical dialects, complicating the detection process.\\n\\n6. **Overlapping Signals**: Signals may overlap with intraspecific, interspecific, or unrelated sounds, making it difficult to extract the desired information, especially during events like the dawn chorus when many birds sing simultaneously.\\n\\n7. **Need for Human Investment**: Despite advancements in automation, a substantial amount of human effort is required to create training datasets and evaluate detection systems.\\n\\nThese challenges highlight the need for robust models that can accommodate variations and the importance of pilot studies to assess feasibility before large-scale implementation (as discussed in the document's introduction and practical guide sections).\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      # Contribution of Machine Learning Algorithms to Bioacoustics\\n\\nMachine learning (ML) algorithms contribute to bioacoustics primarily through the development of automatic detection systems that can analyze and interpret bioacoustic data. These algorithms, particularly deep neural networks, have the potential to significantly advance the field by improving processing times and enabling new applications in bioacoustics. \\n\\nHowever, there are challenges associated with their implementation, including the need for substantial computational resources and the complexity of training models on bioacoustic data. The document notes that the collaboration between biologists and ML developers is essential to address the specific challenges faced in bioacoustics, ensuring that the solutions developed are relevant and effective for practitioners in the field.\\n\\nAdditionally, the document highlights that while there are impressive advances in ML, the field of bioacoustics still faces barriers to entry for newcomers, which may hinder the broader application of these technologies (referenced in the sections discussing accessibility and future directions).\n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     # Real-World Applications of Automatic Detection in Bioacoustics\\n\\nAutomatic detection in bioacoustics has several real-world applications, including:\\n\\n1. **Monitoring Acoustically Active Animals**: Automatic detection can be used to identify and monitor various animal species based on their acoustic signals, such as birdsong or mammal calls.\\n\\n2. **Spatial Habitat Use**: Researchers can utilize automatic detection to analyze how different species use their habitats over time, providing insights into ecological patterns.\\n\\n3. **Behavioral Analysis**: The technology can facilitate the study of animal behavior by detecting specific vocalizations or sounds associated with particular activities.\\n\\n4. **Scaling to Large Data Sets**: Automatic detection allows researchers to handle larger spatiotemporal data sets, enhancing the scope and scale of ecological and conservation studies.\\n\\nThese applications highlight the potential benefits of automatic detection in addressing important evolutionary, ecological, and conservation questions (Section III of the context).\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0                                        Automatic detection in bioacoustic research plays a vital role by enabling researchers to analyze vast amounts of audio data efficiently. It allows for the identification of species, tracking population density, and observing behavioral patterns without direct human observation. This technology has revolutionized ecological studies by making passive monitoring possible, especially in remote or otherwise inaccessible environments, thus contributing to conservation efforts and biodiversity studies.\n",
       "1                                     Automatic detection facilitates ecosystem monitoring by analyzing acoustic indices that reflect biodiversity and environmental changes. Through continuous audio data collection, this technology can identify shifts in species occupancy, variations in vocal activity, and even detect the presence of invasive species. This approach aids in assessing ecosystem health and can alert researchers to changes that may indicate broader environmental shifts, offering a proactive approach to ecosystem management.\n",
       "2    Challenges in implementing automatic detection systems include dealing with high levels of noise in natural environments, variability in animal vocalizations, and the need for large datasets for accurate machine learning models. Field conditions often result in suboptimal recording quality, and the diversity of animal calls requires robust algorithms to avoid misidentification. Additionally, training datasets must be accurately labeled, which can be a labor-intensive process, especially for cryptic species or infrequent vocalizers.\n",
       "3                                                                    Machine learning algorithms analyze vast amounts of audio data in bioacoustics by identifying patterns and classifying sounds more efficiently than manual methods. These algorithms can learn to recognize species-specific calls, distinguish between environmental noises and biological sounds, and even adapt to new sounds with continued learning. Their application is essential in reducing human error and processing time, especially in large-scale biodiversity assessments.\n",
       "4                                                                                                    Automatic detection is widely applied in conservation, population monitoring, and behavioral studies. For example, detecting and tracking endangered species vocalizations in dense habitats helps conservationists assess population sizes without direct interference. Additionally, it assists in monitoring migration patterns, seasonal behaviors, and the impact of human activities on wildlife by continuously capturing acoustic data over time.\n",
       "Name: Answer, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check visually by human\n",
    "with pd.option_context('display.max_colwidth', None):\n",
    "    display(predictions, test_questions[\"Answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the predictions with the real answers\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "test_results = rouge.compute(predictions=predictions,\n",
    "                                       references=test_questions[\"Answer\"],\n",
    "                                       use_aggregator=True,\n",
    "                                       use_stemmer=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3009109577638095,\n",
       " 'rouge2': 0.06685732525035737,\n",
       " 'rougeL': 0.16918138706750935,\n",
       " 'rougeLsum': 0.19027186916605593}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_project_genai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
